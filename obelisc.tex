% Unofficial ERC Starting Grant LaTeX template 
% Source: http://www.arj.no/2013/02/03/erc-stg-latex/
%

\documentclass[oneside, a4paper, onecolumn, 11pt]{article}

%%% PACKAGES %%%

\usepackage{times}
\usepackage[left=2cm,top=1.5cm,bottom=1.5cm,right=2cm]{geometry}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{graphicx} 		% Add graphics capabilities
\usepackage{amsmath}  		% Better maths support
\usepackage{cite}	% bibliography style
\usepackage{eurosym}
\usepackage[table]{xcolor}
\usepackage{xurl}
\usepackage{longtable}
\usepackage{awesomebox}
\usepackage{xspace}
\urlstyle{rm}
\usepackage{tikz}

% To fix list things: 
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*}
\usepackage{amssymb}
\renewcommand{\labelitemi}{\tiny$\blacksquare$}

\usepackage{nopageno}
\usepackage{enumitem}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt} % Remove line at top
 
\lhead{\textcolor{gray}{\emph{Couteau}}}
\chead{\textcolor{gray}{Part B2}}
\rhead{\textcolor{gray}{OBELiSC}}
\cfoot{\thepage}

\newenvironment{itemize*}%
  {\begin{itemize}%
    \setlength{\itemsep}{0pt}%
    \setlength{\parskip}{0pt}}%
  {\end{itemize}}

\usepackage{enumitem}
\usepackage{titlesec}

\newcommand{\qbox}[1]{{\vspace{-.3cm}\awesomebox[gray]{2pt}{\faQuestionCircle}{gray}{#1}\vspace{-.3cm}}}
\newcommand{\qboxx}[1]{\awesomebox[gray][\abShortLine]{2pt}{\faQuestionCircle}{gray}{#1}}

\newcommand{\qboxEff}[1]{{\vspace{-.3cm}\awesomebox[purple]{2pt}{\faQuestionCircle}{purple}{#1}\vspace{-.3cm}}}

\newcommand{\qboxFea}[1]{{\vspace{-.3cm}\awesomebox[blue]{2pt}{\faQuestionCircle}{blue}{#1}\vspace{-.3cm}}}

\newcommand{\qboxSec}[1]{{\vspace{-.3cm}\awesomebox[olive]{2pt}{\faQuestionCircle}{olive}{#1}\vspace{-.3cm}}}

\newcommand{\blacktr}{\rotatebox[origin=c]{270}{$\blacktriangle$}\;\;}
\newcommand{\OBELiSC}{\textsc{OBELiSC}\xspace}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand\rurl[1]{%
  \href{http://#1}{\nolinkurl{#1}}%
}

%%% MATH %%%

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\G}{\mathbb{G}}
\newcommand{\Enc}{\mathsf{Enc}}
\newcommand{\Ring}{\mathcal{R}}
\newcommand{\Adv}{\mathcal{A}}

%%% OTHERS %%%

\newcommand{\GNote}[1]{\textcolor{magenta}{ \textbf{Geoffroy:} #1 }}

%%% COMMANDS %%%

% Define the title, author and date of the document.
\title{ACRO:\\ }
\author{My name\\ My university}

%%%% numbering sections with letters %%%%%%%

\newcounter{alphasect}
\def\alphainsection{0}

\let\oldsection=\section
\def\section{%
  \ifnum\alphainsection=1%
    \addtocounter{alphasect}{1}
  \fi%
\oldsection}%

\renewcommand\thesection{%
  \ifnum\alphainsection=1% 
    \alph{alphasect}%
  \else%
    \arabic{section}%
  \fi%
}%

\newenvironment{alphasection}{%
  \ifnum\alphainsection=1%
    \errhelp={Let other blocks end at the beginning of the next block.}
    \errmessage{Nested Alpha section not allowed}
  \fi%
  \setcounter{alphasect}{0}
  \def\alphainsection{1}
}{%
  \setcounter{alphasect}{0}
  \def\alphainsection{0}
}%

%%% BODY OF THE DOCUMENT: %%%
\begin{document}
\setlength{\aweboxleftmargin}{0.08\linewidth}
\setlength{\aweboxcontentwidth}{0.92\linewidth}

\noindent
%European Research Council
\begin{center}
\large{\textbf{ERC Starting Grant 2023 -- Part B2 }
}\vspace{-.2cm}
\end{center}

\begin{alphasection}
\titlespacing*{\subsubsection}{0pt}{2ex}{1ex}
\titlespacing*{\paragraph}{0pt}{1.5ex}{1ex}
\section*{Section a. State-of-the-art and objectives}
%\setcounter{section}{1}

Privacy-preserving communication has become the norm over large-scale communication networks such as the Internet: since 2020, around 85\% of the total Internet traffic is encrypted~\cite{fortinet}. However, our use of these networks is also evolving rapidly, along with the development of new technologies. The modern user of a large-scale network searches through pictures stored in the Cloud, gets video recommendations on their favorite streaming platform, sees targeted advertising, and uses healthcare apps, dating apps, and social networks. In each of these situations, a third party (the Cloud, the website, \emph{etc}) is \textbf{performing a computation on the private data} of the user (their pictures, their navigation history, their preferences, \emph{etc}). Because these modern uses are part of our everyday lives, we are strongly \textbf{incentivized} to let third parties access our private data, which has led to numerous scandals over the years, with companies selling private user data, or sensitive databases leaking after breaches. Furthermore, access to a large database of private data is highly beneficial to machine learning algorithms, which hold great promises. All of this is at odds with the fundamental importance of \textbf{maintaining individuals' data privacy}.\\[-.1cm]

\fcolorbox{black}{gray!20}{\begin{minipage}{.9 \textwidth}
The ultimate goal of \OBELiSC is to enable the construction of networks where \textbf{privacy} is guaranteed by default, even when the private data is \textbf{used in computations} involving third parties.
\end{minipage}}

\subsection{Secure Computation}
\label{subsec:seccomp}
Secure computation (also called multiparty computation, or MPC) is the branch of cryptography that studies the design of methods to execute computations on sensitive data held by multiple parties, whithout compromising the data privacy. It was introduced in the seminal works of Yao~\cite{FOCS:Yao82b} and of Goldreich, Micali, and Wigderson~\cite{C:GolMicWig86}. In a secure computation protocol, each participant has an input $x_i$. It allows all participants to jointly reveal to a subset of them the value $y = f(x_1, \cdots, x_n)$, where $f$ is a public function, \emph{while concealing all information about $(x_1, \cdots, x_n)$} beyond $y$.

The target function $f$ is commonly represented as a boolean circuit, \emph{i.e.} a directed acyclic graph with indegree two where the internal nodes are called \emph{gates} and compute basic boolean operations, such as XORs or ANDs. There are currently three main paradigms for the design of secure computation protocols:
\begin{itemize}
    \item \textbf{Secret-sharing-based} protocols build upon the seminal protocol of~\cite{C:GolMicWig86}. They are the most lightweight in terms of computation. They usually have communication proportional to the size of the circuit and require a number of rounds of interaction proportional to the depth of the circuit. Furthermore, they can be efficiently \emph{preprocessed}: all cryptographic operations can be pushed to a one-time \emph{preprocessing phase}, independent of the inputs. The online phase, where the actual computation takes place, is extremely lightweight (a few bits of communications and a few boolean operations per gate of the circuit).
    \item \textbf{Garbled-circuit-based} protocols build upon the seminal protocol of Yao~\cite{FOCS:Yao82b}. They usually require more communication and computation compared to their secret-sharing-based counterpart. However, they only require a constant number of rounds of interaction, making them a good choice over high latency networks. Some recent protocols (such as~\cite{CCS:WanRanKat17a}) combine these two paradigms to get the best of both worlds.
    \item \textbf{FHE-based} protocols build upon a cryptographic primitive called \emph{fully homomorphic encryption}, introduced in a 2009 breakthrough~\cite{STOC:Gentry09}, to achieve an extremely low communication footprint (proportional only to the size of the inputs and outputs of the function) in a small number of rounds. This comes at the cost of using a considerably larger amount of computation, with heavy cryptographic operations for every gate of the circuit. 
\end{itemize}

\paragraph{Notions of security.} In secure computation, the security of a protocol for computing a function $f$ between $n$ parties $(P_1, \cdots, P_n)$, with respective inputs $(x_1, \cdots, x_n)$, is defined by considering an \emph{adversary} $\Adv$, which can \emph{corrupt} a subset of the parties. When a party is corrupted, $\Adv$ sees everything they see: their input, their private coins, and all messages they receive throughout the protocol. The protocol is secure if everything $\Adv$ can learn about the private inputs $(x_1, \cdots, c_n)$ can be efficiently computed from the inputs $(x_i)_{i \in \mathsf{C}}$ of the corrupted parties $\mathsf{C}$, and the output $y = f(x_1, \cdots, x_n)$ (this is formalized by requiring that the distribution of everything $\Adv$ sees can be efficiently \emph{simulated} given $y$ and $(x_i)_{i \in \mathsf{C}}$). the two standard settings are the \emph{honest-but-curious} setting, where the adversary observes the view of all corrupted parties, but the parties follow the specifications of the protocol, and the \emph{malicious} setting, where the adversary fully controls the corrupted parties, and can make them deviate arbitrarily from the specifications of the protocol. The honest-but-curious setting often serves as an important first step in the design of a secure computation protocol, as several techniques and compilers can be used to enhance honest-but-curious protocols to the malicious setting. The project \OBELiSC will consider both \textbf{the honest-but curious and the malicious settings}, generally by first targeting the honest-but-curious setting, then addressing the extension of the protocols to the malicious setting. Eventually, the \emph{corruption threshold} $t$ is the maximum number of parties that $\Adv$ can corrupt. In the project \OBELiSC, we will always focus on the \textbf{largest corruption threshold} of $t = n-1$, which is the most desirable (any lower threshold requires trusting that the other participants will not collude to break security) and the most challenging.

\paragraph{Core features of secure communication protocols.} The literature on secure computation is particularly dense, and hundreds of general or specialized protocols have been proposed over the years. The guiding principle of OBELiSC, and the PI's research in general, is to focus on secure computation protocols with features that \textbf{mimic the features of secure communication networks}. Indeed, the latter are already widely deployed and in use. Our central thesis is that secure computation protocols need to ultimately achieve comparable features before they can be deployed and used routinely over large-scale networks. Therefore, we will briefly overview the core features of secure communication protocols, and present the state-of-the-art in secure computation through the lens of these core features.

\begin{itemize}
    \item Secure communication protocols have a \textbf{two-phase structure}: a \textbf{key-exchange} phase followed by an \textbf{encryption} phase. In the key-exchange phase, two parties will establish a shared encryption key. This phase uses heavy public-key operations (\emph{e.g.} elliptic curve multiplications) but needs only be executed once, ahead of time. Furthermore, a constant amount of communication in this phase suffices to enable an arbitrary amount of communication afterward. The encryption phase uses the shared short key to encrypt an arbitrary-length message. This process requires symmetric-key operations, which are much cheaper than public-key operations (around three orders of magnitude). Hence, encrypting an arbitrary-length string requires only a one-time constant-communication heavy phase, followed by cheap operations.
    \item Secure communication protocols have a \textbf{non-interactive structure} in the key exchange phase. Concretely, $n$ parties can at any time publish online a short public key, and keep a local state (their secret key). Any pair of parties can afterward complete the key exchange phase without further interaction, by locally deriving a shared key from their secret key and the other party's public key. Therefore, over an $n$ party network, executing all key exchange phases requires only $O(n)$ communications (instead of $O(n^2)$), and does not require pairs of parties to be simultaneously online.
    \item Secure communication protocols have a \textbf{minimal communication overhead} compared to insecure communications: the total communication of a $t$-bit message scales only as $t + O(1)$. Hence, using secure communications instead of insecure communications has a vanishing impact on bandwidth use.
    \item Eventually, secure communication protocols can be based on a \textbf{variety of established assumptions}, with ongoing standardization efforts allowing to increase the diversity of available constructions under different assumptions. This flexible choice of the security foundation increases our trust that our networks can adapt to unforeseen algorithmic and technological breakthroughs.
\end{itemize}

\subsection{State of the Art in Secure Computation, and Objectives}
\label{sub:sota}

\subsubsection{Secure Computation with Silent Preprocessing}
\label{subsub:silent}
The silent preprocessing paradigm parallels the two-phase structure of secure communication: a short, one-time, ahead-of-time interaction (whose cost is independent of the size of the function to be computed) enables two parties to generate \textbf{correlated keys}. Later, these keys can be locally stretched into long pseudorandom correlated strings using cheap operations. Eventually, these long correlated strings can be used to execute the (very lightweight) online phase of a secret-sharing-based protocol. This is analogous to how a one-time key-exchange allows two parties to generate a short shared key, which can be used to encrypt arbitrarily long strings using cheap operations. Hence, silent preprocessing allows secure computation to achieve the \textbf{first core feature} outlined above.

\paragraph{Genesis.} The seminal work of Beaver~\cite{C:Beaver91b,C:Beaver95} was the first to observe that all expensive operations could be pushed into a preprocessing phase, independent of the inputs, in the secret-sharing-based protocol of~\cite{C:GolMicWig86}. Later, Beaver further observed~\cite{STOC:Beaver96a} that it was theoretically possible to confine all the computation of this preprocessing phase to a constant amount of expensive (public key) operations, followed only by cheap (symmetric key) operations. Ishai, Kilian, Nissim, and Petrank~\cite{C:IKNP03} later demonstrated that this confinement could be executed with very high concrete efficiency. Their breakthrough result had a fantastic impact on the construction of efficient MPC protocols in the two decades that followed.

The work of~\cite{C:IKNP03} comes very close to achieving the same two-phase structure as secure communication, with one key difference: the preprocessing phase still requires the total communication to scale \emph{with the size of the target function} (in contrast, the communication of the key exchange phase in secure communication is \emph{independent} of the size of the string to be transmitted). Intuitively, this gap stems from the higher complexity of the preprocessing material. Concretely, for secure communication, it suffices for two parties to share the \emph{same} short key (for example, this key can be expanded into a long pseudorandom string, and used as a one-time pad to mask the message). in contrast, for secure communication, the parties will require \emph{different} pseudorandom strings, that \emph{satisfy a specific correlation} (but appear random to their opponent otherwise). Let us give an example, which suffices for fast two-party secure computation of functions with at most $s$ AND gates in the honest-but-curious security model: Alice gets random length-$s$ bit-vectors $\vec{a}_0, \vec{a}_1$, and Bob gets random $\vec{b}_0, \vec{b}_1$ such that $\vec{a}_0+\vec{b}_0$ is the bitwise product of $\vec{a}_1$ and $\vec{b}_1$. This is called the \emph{oblivious transfer correlation}.

The technical challenge is therefore the following: finding a way to distribute \emph{short, correlated keys} to Alice and Bob such that locally (``silently'') expanding these keys produces long, random-looking strings that nonetheless satisfy a global correlation (such as the one above). This object, a \emph{pseudorandom correlation generator} (PCG), requires a delicate equilibrium: one must design a process that creates pseudorandomness (hence destroys all apparent local structures) while preserving some global structure. A template to achieve this feature was first introduced in 2017 by the PI~\cite{CCS:BCGIO17}, using the recently-introduced powerful notion of \emph{homomorphic secret sharing} (HSS). However, despite optimization efforts, the concrete efficiency of this approach was low. A major step was made a year later, in another work of the PI~\cite{CCS:BCGI18}, which introduced the first PCG for a useful, albeit restricted, type of correlation. Subsequently, the PI and its coauthors described the first full-fledged PCG for the oblivious transfer correlation in~\cite{C:BCGIKS19} and improved its efficiency and security properties in~\cite{CCS:BCGIKRS19}.

\paragraph{Advanced constructions.} The latest results in this area, both by the PI~\cite{C:CouRinRag21,C:BCGIKRS22} and other teams~\cite{CCS:YWLZW20,SP:WYKW21} made the computational efficiency of PCG-based secure computation with silent preprocessing on par with the protocol of~\cite{C:IKNP03}, yielding overall a major reduction in the communication complexity of many secure computation protocols, without sacrificing computational efficiency. The focus of these results is however on a specific setting, and they suffer from some limitations:
\begin{itemize}
    \item They target the oblivious transfer correlation, which is the natural target for computing \emph{boolean} circuits, in the honest-but-curious setting. However, when targeting a stronger adversarial model (such as the malicious setting), or when computing functions which are best represented as an arithmetic circuit (where operations are over a larger ring $\R$), other correlations are required.
    \item They allow to locally expand \emph{all at once} the short keys into long correlated pseudorandom strings. In contrast, secure communication protocols allow to reuse indefinitely the short shared key and to encrypt on-demand strings of arbitrary length.
    \item They are inherently limited to the two-party setting.
\end{itemize}

%\GNote{look also at assumptions?}

Each of these shortcomings has motivated further research in the construction of advanced forms of PCGs. The case of secure computation of arithmetic circuits, and in the malicious setting, was addressed by the PI in~\cite{C:BCGIKS20}. This works, however, is limited to MPC over \emph{large} rings $\R$. It also requires a more exotic assumption, has a much larger correlated key size, and incurs a superlinear computational cost due to the use of high-dimensional fast Fourier transforms. The bounded nature of PCGs was addressed by the PI in~\cite{FOCS:BCGIKS20}, where the notion of pseudorandom correlation \emph{functions} was introduced to capture the ability to generate unbounded amounts of correlated randomness. Here again, the underlying assumption is new and exotic, and the concrete efficiency is much lower than that of the state-of-the-art constructions of~\cite{C:CouRinRag21,C:BCGIKRS22}. Eventually, scaling to more than two parties was addressed in~\cite{C:BCGIKS20,C:BCGIKRS22}, with the design of \emph{programmable} PCGs. In short, a programmable PCG allows to combine $n^2$ instances of a two-party PCG into an $n$-party PCG. While this allows scaling to more than two parties, the $n^2$ costs limits this approach to small values of $n$, and programmable PCGs also suffer from the typical limitations of advanced PCGs: exotic assumptions, and lower concrete efficiency.

\paragraph{Objectives.} Secure computation with silent preprocessing must \textbf{become usable in the context of large-scale networks}: for general functions (represented as boolean circuits, arithmetic circuits, or even mixtures thereof), with multiple participants, with security against malicious adversaries, and with good concrete efficiency. The project \OBELiSC will strive to lift all remaining barriers on the path to this goal.

\subsubsection{Non-Interactive Secure Computation}
\label{subsec:ni}
In a secure computation protocol, the parties exchange messages back and forth. Therefore, to securely compute a function in a timely fashion, the parties must coordinate among themselves to be online at the same time. This is at odds with the natural use of communication networks, where parties locally decide when to be online or offline, without coordination. Indeed, the \textbf{non-interactive structure} of secure \emph{communication} is precisely what circumvents this pitfall: it removes the need for being online simultaneously. Achieving the same for secure computation, or understanding when it is at all possible is the goal of \textbf{non-interactive secure computation} (NISC). Informally, a non-interactive protocol has the following two-step structure: (step~1) each party $P_i$ with input $x_i$ broadcasts $m_i = \Enc(x_i;r_i)$, where $\Enc$ is some encoding function, and $r_i$ denotes the random coins; (step~2) given $(m_1, \cdots, m_n)$ and $(x_i,r_i)$, each party $P_i$ can compute its output $f_i(x_1, \cdots, x_n)$, without further communication. The protocol can also possibly allow to evaluate distinct functions $f_i$, possibly using subsets of the messages $(m_1, \cdots, m_n)$.

This is the same structure as secure communication protocols, where all parties publish online their public key, and can locally decode a shared key given the public key of another party. It is not too hard to prove that NISC is impossible to achieve for arbitrary functionalities in the multiparty setting. Characterizing \emph{which} functionalities can have NISC protocols, and how efficient these protocols can be, has been thoroughly studied, both in the two-party setting~\cite{EC:IKOPS11,EC:AMPR14,EC:MohRos17,AC:BJOV18,C:CDIKLOV19,CCS:BCGIKRS19} and in the multi-party setting~\cite{C:BGIKMP14,AC:HIJKSY17,C:BenKraRab17,EC:MorPasPol20,TCC:BenLin20,C:EOYN21,EC:BJKL21}. Circumventing the lower bounds can be done by relaxing the security notion~\cite{C:BGIKMP14,C:EOYN21}, allowing the adversary to arbitrarily evaluate the residual function (where the honest parties inputs are hardcoded) on multiple inputs. Another natural approach is to \textbf{target restricted functionalities} of interest: this will be the approach taken in \OBELiSC. More precisely, \OBELiSC will target two functionalities:
\begin{itemize}
    \item \textbf{Silent preprocessing protocols} will be the first main target. Indeed, a non-interactive silent preprocessing protocol would achieve the \textbf{first two core features} of secure communication protocols, representing a major step on the path to the ultimate goal of \OBELiSC.
    \item \textbf{Zero-knowledge proofs} (ZKP) will be our second main target. ZKP are already widely used in the real-world for tasks such as electronic voting~\cite{helios,belenios}, anonymous authentication~\cite{AC:Chaum90}, or anonymous transactions~\cite{FC:BAZB20,monero}, among others. ZKP is perhaps the form of secure computation with the most potential for immediate application and deployment, and standardization efforts are already underway~\cite{zkstandard}. Hence, advances in the design of zero-knowledge proofs have the potential to yield \textbf{immediate tangible benefits} and to be deployed over existing networks.
\end{itemize}

\paragraph{NISC for silent preprocessing.} A two-party NISC protocol for silent preprocessing would yield a major breakthrough: it would enable the execution of silent preprocessing protocols among $n^2$ pairs of participants while incurring only a $O(n)$ communication overhead, analogous to how secure communication requires only a $O(n)$ amount for communication (each party uploads its public key) for all pairwise key-exchanges. Since the ultimate goal is to enable a privacy-preserving use of large-scale networks, where $n$ can be very large, this is a must-have. However, NISC for silent preprocessing is non-trivial. Beyond purely theoretical solutions, there was no concrete proposal to achieve it until the very recent breakthrough work of~\cite{EC:OrlSchYak21}, which constructed the first true NISC for silent preprocessing. However, despite its ingeniosity, this approach remains considerably slower than traditional interactive silent preprocessing protocols, by several orders of magnitude, due to its heavy use of expensive cryptographic operations over large composite-order rings.

\paragraph{Non-interactive zero-knowledge proofs.} Zero-knowledge proofs were introduced in the seminal paper of Goldwasser, Micali, and Rackoff~\cite{GolMicRac89}. A zero-knowledge proof of knowledge is a two-party protocol between a \emph{prover}, holding a proof $\pi$ for a public statement, and a \emph{verifier}, who wish to be convinced that the prover knows a valid proof. At the end of the interaction, the verifier should be convinced if and only if the prover was honest, yet should not learn any information about $\pi$ beyond its existence (and the fact that the prover knows it). A \emph{non-interactive} zero-knowledge proof~\cite{STOC:BluFelMic88} (NIZK) consists of a single prover message, and any party of the network can play the role of the verifier to check the proof. Non-interactivity is a crucial feature whenever zero-knowledge is used over a large network: without it, the prover would have to re-execute the ZKP with all members of the network (a standard example is electronic voting, where each voter will send an encrypted vote and append a proof that the ciphertext encrypts a valid vote, which every other voter can verify).

The literature on NIZKs is too vast to be covered in full in this short overview. At a very high level, though, the study of NIZKs falls into three broad categories, from the most practical to the most theoretical:

\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
    \item Building \textbf{very efficient} NIZKs, either for general statements (represented by circuits) or for concrete statements of interest. The NIZKs in this category usually sacrifice provable reductions to established assumptions, and instead, rely on heuristic (but well-studied) security arguments. Concretely, these NIZKs typically build upon an interactive ZKP, which is then compiled into a non-interactive proof using a hash function (this is the \emph{Fiat-Shamir transform}~\cite{C:FiaSha86}); the latter can be heuristically argued to provide a secure proof by showing that it is provably secure when the hash function is modeled as a true random oracle. Alternatively, NIZKs in this category can also rely on exotic \emph{knowledge-of-exponent} assumptions. Important results in this direction includes general proofs such as SNARGs and their many variants~\cite{SP:BBBPWM18,CCS:MBKM19,C:Setty20}, and optimized proofs for specific statements~\cite{EC:BayGro13,EC:GroKoh15,CCS:KatKolWan18}, an area where the PI had several significant contributions~\cite{EC:CouPetPoi17,EC:CKLR21,CCS:CGKR22}. Notably, an important effort is devoted to constructing efficient NIZKs that plausibly withstand \emph{quantum adversaries}~\cite{CCS:KatKolWan18,PKC:BdKOSZ21,C:FenJouRiv22}.
    \item Building \textbf{reasonably efficient} NIZKs whose security reduces to well-established assumptions (without going through the Fiat-Shamir heuristic). This category typically relies on pairing-based cryptography. The NIZKs in this category are usually less concretely efficient than heuristic NIZKs and induce much larger proofs than NIZKs using knowledge-of-exponent assumptions. Yet, they achieve a very interesting tradeoff by providing decent efficiency, and strong security foundations. The seminal work in this area is the Groth-Sahai framework~\cite{EC:GroSah08} and its follow-ups~\cite{ACNS:BFIJSV10,PKC:EscGro14,TCC:Rafols15,PKC:DGPRS19}. Recent developments include a new alternative framework introduced by the PI~\cite{C:CouHar20,AC:CLPO21}, and low-communication pairing-based NIZKs for general statements~\cite{STOC:KalPanYan19, C:WatWu22}.
    \item Expanding the set of \textbf{standard assumptions} which are known to imply the existence of NIZKs. This is the most theoretical category, whose aim is to put the existence of NIZKs on firm grounds, by demonstrating that they can be constructed from any of the most standard cryptographic assumptions. The seminal works in this category are~\cite{STOC:BluFelMic88,FOCS:FeiLapSha90}. Recently, exciting progress has been made, leading to the construction of NIZKs under traditional lattice-based assumptions~\cite{STOC:CCHLRRW19,C:PeiShi19}. For constructions using traditional, discrete logarithm-style assumption, after a sequence of exciting progresses~\cite{EC:CCRR18,STOC:CCHLRRW19,EC:CouKatUrs20,C:BraKopMou20} to which the PI contributed on several occasions~\cite{EC:CouHof19,EC:CouKatUrs20,TCC:CKSU21}, a major step was recently achieved in~\cite{EC:JaiJin21}.
\end{enumerate}

\paragraph{Objectives.} The first main goal will be to \textbf{achieve concretely efficient non-interactive silent preprocessing}. This is a two-step goal: the first target will be to achieve this for simple correlations, such as the oblivious transfer correlation, for which very efficient interactive silent preprocessing protocols are already known. Then, when some of the objectives of Section~\ref{subsub:silent} are reached and provide new silent preprocessing protocols for stronger forms of correlations, and general arithmetic circuits, the next target will be to enhance these protocols with non-interactivity.

For non-interactive zero-knowledge, \OBELiSC will address both the construction of concretely efficient NIZKs that build upon the Fiat-Shamir heuristic (category~1 above) and the question of lifting the remaining barriers towards getting a complete understanding of the security foundations of NIZKs (category~3 above). In the former setting, \OBELiSC will mostly target concrete statements which are especially useful in real-world interactions. One such example is \textbf{range proofs}, which demonstrates that a secret value is within appropriate bounds: range proofs are a core component in anonymous cryptocurrencies, anonymous transaction systems, and some e-voting protocols. Other concrete statements of interest include \textbf{membership proofs} (which are used in anonymous authentication to prove membership to a whitelist of authorized users) and proofs of \textbf{knowledge of a preimage to a hash function} (which are used in the design of efficient signature schemes). While very efficient proofs are known for the first two statements~\cite{EC:GroKoh15,SP:BBBPWM18,CCS:CGKR22}, they do typically not achieve post-quantum security (in contrast, known silent preprocessing protocols typically achieve post-quantum security ``natively''). Recently, an important effort has been devoted to building new post-quantum proofs for the last statement above, the knowledge of a hash function preimage, following the NIST post-quantum standardization process. However, these protocols are very much in their infancy, and there seems to be a lot of room for improvement, especially in the setting of code-based constructions (this was recently acknowledged by the NIST, who announced its desire to re-open a new standardization process to get better post-quantum signatures, in particular from code-based assumptions). Post-quantum security is a must-have for preparing future secure computation networks against the possible advent of quantum computers. Therefore, \OBELiSC will also focus on building \textbf{new efficient NIZKs with post-quantum security} for useful statements.

Eventually, in the ``category~3'' setting, the project \OBELiSC will strive to understand what are the \textbf{minimal assumptions necessary to construct NIZKs}. The most immediate question along these lines is to look at traditional assumptions, such as (say) the decision Diffie-Hellman assumption, and try to build NIZKs under these assumptions (the recent breakthrough of~\cite{EC:JaiJin21} comes close to achieving this, but requires assuming security of the assumption against subexponential adversaries). This is already a high-risk goal, as basing NIZKs on new assumptions has proven to be highly non-trivial. But the most intriguing, high-risk high-gain goal, will be to answer what is perhaps the most fundamental question about the security of NIZKs: \textbf{is public-key cryptography necessary for NIZKs}? Traditional (interactive) ZKP require only minimal ``symmetric key'' assumptions (such as the existence of one-way functions), and heuristic constructions of NIZKs with Fiat-Shamir do not seemingly require structured, public-key-style, cryptographic primitives. Yet, \emph{all known provably secure NIZK constructions} of the past four decades have always required the power of public-key cryptographic assumptions. Whether this is inherent is a major open question.

\subsubsection{Secure Computation with Low Communication}
\label{subsub:lowcomm}

For a long time, all standard approaches to secure computation have been stuck at an intriguing \emph{circuit-size barrier}: they required an amount of communication proportional to the size of the circuit being computed. In contrast, insecure computation requires much less communication: $n$ parties with inputs $(x_1, \cdots, x_n)$ can simply broadcast their input, allowing every party to evaluate the target function. For many functions of interest, the size of the inputs can be considerably smaller than the entire circuit representation of the target function.

Getting beyond this limitation has been a major challenge in secure computation for several decades. \iffalse Early positive results required exponential computation~\cite{C:BFKR90,STOC:NaoNis01}, or were limited to very simple functions such as point functions~\cite{FOCS:CGKS95, FOCS:KusOst97, STOC:ChoGil97} or constant-depth circuits~\cite{C:BarIsh05}.\fi The situation changed with the breakthrough result of Gentry~\cite{STOC:Gentry09} on fully homomorphic encryption (FHE), which led to optimal communication protocols in the computational setting~\cite{TCC:DamFauHaz12,EC:AJLTVW12}. While the advent of FHE was a major breakthrough for low-communication secure computation, it has several limitations. First, the set of assumptions under which we know how to build FHE is very narrow: it is restricted to lattice-based assumptions such as LWE, and in particular, does not include any of the traditional assumptions which were used in the 20th century. Second, and despite tremendous improvements in the past one-and-a-half decade, it remains considerably less efficient than the other standard approaches to secure computation.

In 2016, a new step was taken in the work of~\cite{C:BoyGilIsh16}, which demonstrated the possibility of achieving sub-circuit-size communication for secure computation without relying on FHE (under the traditional decision Diffie-Hellman assumption). Subsequently, several works of the PI~\cite{EC:Couteau19, EC:CouMey21, TCC:BoyCouMey22}, and others of~\cite{EC:OrlSchYak21,C:RoySin21}, expanded the set of assumptions known to imply sublinear secure computation. Notably, the works of~\cite{EC:Couteau19, EC:CouMey21} demonstrated that sublinear communication could be achieved \textbf{within the framework of MPC with silent preprocessing}. Albeit mostly theoretical, these results demonstrate that a low communication overhead can in principle be achieved even while retaining the core features of silent preprocessing.

\paragraph{Objectives.} An important research effort is currently devoted to optimizing and improving fully homomorphic encryption, both by researchers and by companies. The project \OBELiSC will investigate the complementary research direction, which has received considerably less attention so far: studying and developing new paradigms to obtain low-communication secure computation protocols without going through FHE. Diversifying the approaches to sublinear secure computation has immediate benefits: it strengthens its security foundations, by basing sublinear MPC on a larger set of traditional assumptions. A more high-risk high-gain motivation of project \OBELiSC is to ultimately achieve \textbf{concretely efficient low-communication MPC protocols}, by developing new methods to circumvent the circuit-size barrier. The ultimate goal will be to achieve concretely efficient low-communication MPC protocols with \textbf{silent preprocessing}, or even with \textbf{non-interactive silent preprocessing}. This would finally make secure computation on par with secure communication feature-wise, a prerequisite for someday deploying large-scale networks where privacy-by-default is the rule whenever user data is exchanged and used in computations.

\section*{Section b. Methodology}
\setcounter{subsection}{0}

We explain how we plan to tackle the objectives outlined in Section~\ref{sub:sota}. We do so by introducing concrete research questions to address and expanding upon possible directions and techniques to tackle each of these questions. The project \OBELiSC will tackle questions of very different natures. To provide a clearer overview, we group them into three broad categories, and use a color code to denote the category of each question:\\[-.3cm]

\begin{itemize}[leftmargin=+.3in]
    \item[\textcolor{purple}{\faQuestionCircle}]: the red question mark denotes questions about improving the \textbf{efficiency} of cryptographic protocols.
    \item[\textcolor{blue}{\faQuestionCircle}]: the blue question mark denotes questions about the \textbf{feasability} of certain cryptographic primitives and protocols, when their existence is open.
    \item[\textcolor{olive}{\faQuestionCircle}]: the olive question mark denotes questions about improving the \textbf{security} of cryptographic protocols (\emph{e.g.} basing them on better understood assumptions).
\end{itemize}
The distribution of the personnel involvement across the goals of \OBELiSC is summarized in Figure~\ref{fig:distribution}.

\setlength{\abovecaptionskip}{-10pt}
\setlength{\belowcaptionskip}{-10pt}
\begin{figure}[hbt]
\begin{center}
\begin{tikzpicture}[yscale=.75]
\filldraw[fill=blue!10] (0,-1) rectangle (15,6);

\draw (1.5,5.6) node{\textbf{Year 1}};
\draw (3,-1) -- (3,6);

\draw (4.5,5.6) node{\textbf{Year 2}};
\draw (6,-1) -- (6,6);

\draw (7.5,5.6) node{\textbf{Year 3}};
\draw (9,-1) -- (9,6);

\draw (10.5,5.6) node{\textbf{Year 4}};
\draw (12,-1) -- (12,6);

\draw (13.5,5.6) node{\textbf{Year 5}};

\filldraw[fill=gray!0, rounded corners] (0,4.6) rectangle node{\textbf{PI: RG1 + RG2 + RG3}} (15, 5.2);
\filldraw[fill=red!5, rounded corners] (0,3.6) rectangle node{\textbf{PhD1: RG1 + RG3}} (9, 4.2);
\filldraw[fill=red!5, rounded corners] (3,2.6) rectangle node{\textbf{PhD2: RG1 + RG2}} (12, 3.2);
\filldraw[fill=red!5, rounded corners] (6,1.6) rectangle node{\textbf{PhD3: RG2}} (15, 2.2);
\filldraw[fill=red!5, rounded corners] (6,0.6) rectangle node{\textbf{PhD4: RG2 + RG3}} (15, 1.2);
\filldraw[fill=green!5, rounded corners] (3,-.4) rectangle node{\textbf{Postdoc: RG1 + RG2 + RG3}} (15, .2);

\end{tikzpicture}
\end{center}
\caption{Distribution of the personnel resources during the project \OBELiSC}
\label{fig:distribution}
\end{figure}

\subsection{Research Goal 1 (RG1): Efficiency and scalability of secure computation with silent preprocessing}
\label{wpone}

Silent preprocessing allows to generate correlated (pseudo)randomness. The choice of the target correlation is guided by the type of function we want to compute. For example, the oblivious transfer (OT) correlation (outlined in Section~\ref{subsub:silent}) is a suitable choice for functions best represented as boolean circuits. For functions best represented as an \emph{arithmetic} circuit (where the gates compute additions and multiplications over a finite field $\F$), a better choice is the \emph{oblivious linear evaluation} (OLE) correlation: Alice gets many random pairs $(a_0, a_1)$ (viewed as the coefficient of the function $x \rightarrow a_0 x + a_1$) and Bob gets many pairs $(b_0, b_1 = a_0b_0 + a_1)$ (the evaluations of each of Alice's functions on independent random points).

\subsubsection{Silent preprocessing for specific functions}
\label{subsub:specific}
One can however be more fine-grained, by looking at \emph{specific} functionalities. Indeed, it is often the case that for concrete functionalities, better efficiency can be achieved by choosing a tailored type of correlated randomness. Some natural examples include:
\begin{itemize}
    \item Linear algebra, which benefits strongly from correlations such as \emph{random matrix products} to speed up secure matrix operations.
    \item Private set intersection (or other privacy-preserving operations between large sets), which are known to benefit strongly from restricted types of OLEs, such as vector-OLE~\cite{EC:RinSch21}, which can be generated more efficiently~\cite{CCS:BCGI18,C:CouRinRag21}.
    \item Machine learning operations, which are often best represented as \emph{mixed-mode} functions, mixing linear algebra operations with boolean operations (such as thresholds), and could benefit from tailored ``mixed-mode'' correlated randomness.
    \item Simple statistics and biometric operations, which can benefit from correlations such as general bilinear correlations, or from inner product correlations~\cite{AC:CouZar22}.
\end{itemize}

The above is just a sample, but already covers many important scenarios, such as the functions naturally occurring in statistical analysis, medical research, or recommendation systems. A natural starting point for RG1 is therefore to construct a toolbox of new silent preprocessing protocols tailored to the generation of application-specific correlated randomness, to achieve greater efficiency in each of these natural applications.

\qboxEff{Can we construct concretely efficient silent preprocessing protocols for correlations tailored to useful functions, such as set operations, comparisons, or functions occurring in statistical analysis, medical research, and recommendation systems?}

\subsubsection{Silent preprocessing for oblivious linear evaluation correlations}
\label{subsub:ole}

A more ambitious goal is to target general forms of correlated randomness, such as the OLE correlation, and to lift the remaining barriers towards making them efficient, usable, and based on strong security foundations. On the efficiency side, the best-known OLE protocols have superlinear computational complexity~\cite{C:BCGIKS20}. This stems from their crucial use of polynomial rings of the form $\Ring = \F_p[X]/F(X)$, where $F(X)$ is a degree-$n$ polynomial (with $n \leq p$) that splits over $\F_p$, to generate size-$n$ OLE correlations. Manipulating elements of $\Ring$ (\emph{e.g.} multiplying them) costs between $O(n\log n)$ and $O(n\log^2n)$ arithmetic operations (depending on $F$). In contrast, silent preprocessing protocols for the OT correlation achieve strictly linear complexity~\cite{C:BCGIKS19,CCS:BCGIKRS19,C:CouRinRag21}, leading to a much better concrete efficiency in practice (one to two orders of magnitude).

\qboxEff{Can we construct silent preprocessing protocols for OLE correlations with linear computational overhead?}

Another severe limitation of the best-known silent preprocessing protocols for OLE is the underlying field size: because these protocols must rely on a ring $\Ring = \F_p[X]/F(X)$ where $F$ splits entirely to generate $\deg(F)$ OLE correlations, they are inherently limited to large fields, satisfying $p \geq n$. It remains a major open question to achieve efficient silent preprocessing for arithmetic circuits over \emph{small} fields.

\qboxFea{Can we construct silent preprocessing protocols for OLE correlations over small fields? Can it be done over the smallest possible field, $\F_2$?}

\paragraph{Methodology.} The field size limitation stems from the use of an assumption on linear code over a polynomial ring $\Ring = \F_p[X]/F(X)$ where $F$ splits entirely. At a high level, the protocol of~\cite{C:BCGIKS20} proceeds by generating a single OLE correlation over $\Ring$, and then projecting it to $\F_p^n$, using the natural isomorphism from $\Ring$ to $\F_p^n$ (where $n = \deg(F)$). A natural target to circumvent the limitation is to study alternative structures $\Ring'$ which are still isomorphic to $\F_p^n$ for a large $n$, without incurring the same limitation on the size of $\F_p$. Algebraic number theory provides multiple candidates: multivariate polynomial rings, Hermitian curves, Carlitz modules, \emph{etc}. Each structure leads to a new candidate code-based assumption, which must be analyzed. In~\cite{FOCS:BCGIKS20,C:CouRinRag21}, a framework to analyze code-based assumptions was put forth by the PI, which reduces their resistance against know attacks to combinatorial properties of the code. A promising approach is therefore to (1) find candidates from the algebraic number theory literature, and (2) use probabilistic arguments (typically, strong concentration bounds on weakly-dependent random variables) to prove that they satisfy the relevant combinatorial properties.

\paragraph{Assumptions for OLE.} Another intriguing question about silent preprocessing for OLE correlations relates to the underlying assumption. The assumption of~\cite{C:BCGIKS20} is somewhat exotic. The approach which we sketched above (towards reaching smaller fields) is promising but is bound to necessitate the introduction of newer, even more exotic code-based assumptions. An even more desirable goal would be to construct a silent protocol for OLE under a well-established assumption.

\qboxSec{Can we construct silent preprocessing for OLE correlations using better-understood assumptions, such as standard code-based assumptions?}

\paragraph{Methodology.} One option is to follow the algebraic number geometry path, but further look for structures such that the code-based assumption on this structure can be proven to be as hard as some standard code-based assumptions. A more open-ended path would be to look for a fundamentally different way to achieve silent protocols for OLE. A starting point could be two recent works of the PI~\cite{FOCS:BCGIKS20,C:BCGIKRS22}. While the context in these works is different, it develops methods to build silent preprocessing protocol using a more combinatorial approach (dispensing with the algebraic structure of~\cite{C:BCGIKS20}). These combinatorial techniques could help with building OLE over small rings, and perhaps avoid the superlinear complexity that stemmed from the use of Fast Fourier Transforms over polynomial rings.

\subsubsection{Unbounded, on-demand silent preprocessing}
\label{subsub:pcf} 

A core difference between silent preprocessing and traditional secure communication protocols, outlined in Section~\ref{subsub:silent}, is the \emph{single use} nature of pseudorandom correlation generator (the cryptographic primitive at the heart of silent preprocessing protocols): they allow to generate once for all an \emph{a priori} bounded amount of pseudorandom correlated randomness. Pseudorandom correlation \emph{functions} (PCF)~\cite{FOCS:BCGIKS20} circumvent this limitation and provide on-demand, unbounded amount of correlated (pseudo)randomness. Yet, the best-known constructions~\cite{FOCS:BCGIKS20,C:BCGIKRS22} suffer from strong caveats: they are much less efficient than their bounded counterparts and rely on entirely new assumptions.

\qboxEff{Can we construct pseudorandom correlation \emph{functions} with efficiency competitive with that of pseudorandom correlation generators?}

\paragraph{Methodology.} There are two natural directions to investigate. The first one starts from the existing constructions introduced by the PI~\cite{FOCS:BCGIKS20,C:BCGIKRS22}: in essence, these constructions use only lightweight operations. Their inefficiency is mostly a consequence of the \emph{parameter choice}: at a high level, the costs involve a $O(\lambda^2)$ factor, where $\lambda$ is a security parameter for the underlying assumption. Unfortunately, the underlying assumptions are new and lack extensive cryptanalysis. In~\cite{FOCS:BCGIKS20,C:BCGIKRS22}, security was analyzed by using a framework developed by the PI in~\cite{FOCS:BCGIKS20} to prove resistance against a large class of attacks, and $\lambda$ was set as the smallest value providing security following this analysis. The analysis in~\cite{FOCS:BCGIKS20,C:BCGIKRS22} apply powerful and generic concentration bounds to complex random variables and are far from tight. It is plausible that a dedicated analysis could uncover much tighter concentration bounds tailored to the scenario at hand (at a high level, one needs new concentration bounds related to the exact distribution of the parities of the ball numbers in balls-and-bins experiments). In turn, this would allow us to isolate a much smaller value of $\lambda$ that suffices to reach acceptable security.

A second approach is to build upon recent results in homomorphic secret sharing (HSS). While HSS-based PCFs are typically inefficient, the recent work of~\cite{EC:OrlSchYak21} showed that minimal use of HSS \emph{in combination} with coding-theoretic constructions can yield new constructions of PCFs where the $O(\lambda^2)$ factor is reduced to a $O(\lambda)$ factor. The construction of~\cite{EC:OrlSchYak21} remains too inefficient for our purpose and targets a very specific form of correlation, but it provides a nice alternative starting point to search for a best-of-both-world construction, enjoying the small asymptotic factors of HSS constructions, but using only (or mostly) the cheap operations of coding-theoretic constructions.

\qboxSec{Can we construct pseudorandom correlation functions under better understood assumptions?}

\paragraph{Methodology.} Here again, a natural starting point would be the second direction above: using coding-theoretic constructions in combination with HSS-based techniques. At a high level, the exotic assumption in~\cite{FOCS:BCGIKS20,C:BCGIKRS22} stems from the need to develop a new coding-theoretic way to generate near-unbounded amounts of structured pseudorandomness, which standard assumptions do not allow. The combination approach, instead, uses an HSS-based technique to generate this near-unbounded amount of pseudorandomness, and (informally) relies on standard coding-theoretic assumption to add structure to this pseudorandomness, leading to the target correlation. Here, the goal will b to study the exact interplay between the underlying assumption, to understand whether this can be achieved for the new efficient constructions we will introduce, and ideally to target security under post-quantum assumptions (which is currently infeasible using the techniques of~\cite{EC:OrlSchYak21}).

\subsubsection{Multiparty silent preprocessing}
\label{subsub:msp}

Eventually, the most ambitious, high-risk goal of RG1 is to obtain the first direct constructions of \emph{multiparty} silent preprocessing protocols. Some protocols introduced by the PI~\cite{C:BCGIKS20,C:BCGIKRS22} enjoy a feature called \emph{programmability}, which states (roughly) that $~n^2$ two-party silent preprocessing protocols can be combined in a single $n$-party protocol. However, this introduces a $O(n^2)$ overhead, which is a very severe limitation: over large networks, scaling with the \emph{square} of the total number of users would be entirely impractical. In contrast, once $n$-party correlated randomness is distributed, the communication complexity of all known modern secure computation protocols scales only with $n$. Furthermore, known \emph{non-silent} preprocessing protocols also generate $n$-party correlated randomness while scaling only with $n$. Towards our goal of a general large-scale secure computation network, achieving this $n$-fold scaling for silent preprocessing is a must-have -- yet, it remained entirely open ever since the introduction of silent preprocessing. Even the core technical tools underlying silent preprocessing protocols (such as the distributed point functions of~\cite{EC:GilIsh14}, or the homomorphic secret sharing techniques based on distributed discrete logarithm~\cite{C:BoyGilIsh16} or local rounding of secret shares~\cite{EC:BoyKohSch19}) are fundamentally stuck at the two-party setting (sometimes with proven barriers~\cite{C:BDIR18}).

\qboxFea{Can we construct $n$-party silent preprocessing protocols with a cost scaling with $n$, for the most useful $n$-party correlations, such as Beaver triples?}

\paragraph{Methodology.} The concrete starting goal will be to first obtain a direct construction of \emph{three-party} silent preprocessing, the minimal step above the two-party barrier, before addressing the general $n$-party case. A \emph{bottom-up} approach is to start from the three-party 'distributed point function' (DPF) of~\cite{EC:BoyGilIsh15}: reducing its $\sqrt{n}$ communication cost $\sqrt{n}$ to $O(\log n)$ would provide a three-party silent preprocessing protocol (using the techniques developed by the PI~\cite{CCS:BCGI18,C:BCGIKS19}). One approach is to isolate which structural properties of the underlying protocol would suffice to get below $\sqrt{n}$, and seek to achieve these properties from structured hardness assumptions (in contrast, the 3-party DPF of~\cite{EC:BoyGilIsh15} relies only on one-way functions: it is not implausible that using more structured assumptions can reduce communication). An alternative \emph{top-down} approach is to start from high-end primitives, such as homomorphic secret sharing (HSS)~\cite{C:BoyGilIsh16} or constrained pseudorandom functions~\cite{AC:BonWat13}. While also limited to two parties, they have stronger structural properties that might allow, with a careful construction, to \emph{nest} them -- \emph{i.e.} getting a $4$-party construction by evaluating a $2$-party construction within a $2$-party construction. This nesting would likely be inefficient at first but could be improved with dedicated optimizations exploiting the simple structure of silent preprocessing.

\subsubsection{Organization over time}

RG1 will span the first four years of project \OBELiSC. We expect two PhD students to work on RG1: one PhD student (year~1 to year~3 of \OBELiSC) will work on $n$-party secure computation when $n$ scales. This includes Section~\ref{subsub:msp} of RG1, as well as part of RG3 (Section~\ref{subsub:more}). A second PhD student (year~2 to year~4 of \OBELiSC) will work on the design of improved silent preprocessing methods. Their focus will be mostly on the goals outlined in Sections~\ref{subsub:ole} and ~\ref{subsub:pcf}, as well as the goals in Section~\ref{subsub:nisp} of RG2 (which has a strong overlap with RG1).  Depending on the candidates, a postdoc might be involved in these aspects.

\subsubsection{Risks and impacts}

Secure computation with silent preprocessing is a fast-growing area, and several important milestones are most likely within reach. The design of application-specific silent preprocessing protocols, discussed in~\ref{subsub:specific}, is of this nature. Yet, it is also a goal with high potential for immediate impact, enabling new \textbf{concretely efficient} protocols usable in real-world scenarios. Improving general silent preprocessing, with OLEs being the main target (Section~\ref{subsub:ole}) is much more challenging. However, there is a well-defined path to improving the state of the art, through the systematic investigation of coding-theoretic assumptions over structures from the algebraic number theory literature. The PI has a strong expertise on silent preprocessing, which he introduced, and achieved most of the previous milestones in the area. On the coding-theoretic and algebraic number theory side, the PI has close collaborations and regular meetings with experts in the area. For example, Alain Couvreur (LIX, INRIA) is a top expert on the use of algebraic number theory in coding theory and is co-supervising a PhD student with the PI. The goal of Section~\ref{subsub:ole} is likely to trigger new collaborations with him and other experts. Achieving new efficient silent preprocessing protocols for OLE could have the most \textbf{wide impact} among the goals of RG1: it would essentially settle the question of building two-party silent preprocessing for all types of functions, a major step towards the end goal of \OBELiSC. Section~\ref{subsub:pcf} contains both low-hanging fruits ---improving existing PCFs by refining their analysis--- and very challenging questions (obtaining the first concretely efficient PCFs). Eventually, Section~\ref{subsub:msp} is the riskiest of RG1, and is particularly ambitious. Due to the expertise of the PI, we believe that new results are achievable. Even very partial results could have a strong impact, by \textbf{providing the first clear indication that silent preprocessing has the potential to scale over large networks}.

\subsection{Research Goal 2 (RG2): Non-interactive secure computation}

\subsubsection{Non-interactive silent preprocessing}
\label{subsub:nisp}

This goal has a strong overlap with RG1: when constructing new silent preprocessing protocols, a natural next question is whether they can be made \emph{fully non-interactive}, which would yield a major breakthrough towards enabling secure computation networks to scale. While this was known to be theoretically feasible (but with impractical proof-of-concept constructions), it is only very recently that~\cite{EC:OrlSchYak21} demonstrated a realistically usable non-interactive protocol for the silent generation of a basic type of preprocessing material. This is an exciting result, but their protocol remains orders of magnitude less efficient than interactive protocols and is limited to the oblivious transfer correlation. A wide, multidirectional question, and a highly non-trivial goal, is to improve this situation, both regarding efficiency and expressivity.

\qboxEff{ Is it possible to construct non-interactive secure protocols for silent preprocessing with better concrete efficiency?}
\vspace{-.3cm}
\qboxFea{ Is it possible to construct non-interactive secure protocols for silent preprocessing for more general types of correlations?}

\paragraph{Methodology.} Silent preprocessing requires stretching pseudorandom-but-structured strings from a short key. A natural approach to non-interactive silent preprocessing would be to evaluate a carefully-chosen pseudorandom number generator (PRG) within a suitable primitive with homomorphic properties to ``add'' the structure afterward. Candidate suitable PRGs are low-depth generators, such as the Goldreich PRG~\cite{EPRINT:Goldreich00b}, which the PI studied in~\cite{AC:CDMRR18}. A suitable primitive will likely be a variant of HSS~\cite{C:BoyGilIsh16} enhanced with non-interactive features. This does not immediately work, as HSS does not natively provide non-interactivity. However, a joint design of a tailored HSS variant with a new low-complexity PRG (according to a complexity measure defined by the properties of the HSS) would be a natural direction to investigate. Another direction to investigate is the oblivious sampling method of~\cite{EC:OrlSchYak21}: rather than \emph{generating} structured pseudorandomness, this works \emph{interprets} truly random public strings as an encoding of structured pseudorandomness. This currently works only with factorization assumptions, which introduces a large overhead since they require very large groups. Investigating whether a similar idea works with different assumptions that avoid these large groups is an intriguing direction.

\subsubsection{Non-interactive zero-knowledge proofs} 
\label{subsub:nizk}
The project \OBELiSC will target two very distinct, but complementary aspects of non-interactive zero-knowledge proofs: \textbf{concrete, real-world efficiency}, and \textbf{theoretical security foundations}. The former is a pressing need, as zero-knowledge proofs are becoming widely used in mainstream products, and with standardization efforts underway~\cite{zkstandard}.

\qboxEff{Can we construct more efficient non-interactive zero-knowledge proofs for statements of interest in real-world applications, such as range proofs and membership proofs?}

\paragraph{Methodology.} In two recent works~\cite{EC:CKLR21,CCS:CGKR22}, the PI introduced a new approach to build efficient NIZK for range statements, based on a standard results in Diophantine equations (which states that an integer $x$ belongs to $[a,b]$ if and only if there exists three integers $x_1,x_2,x_3$ such that $1+4(x-a)(b-x) = x_1^2+x_2^2+x_3^2$). Previous works had to rely on groups of hidden order; unfortunately, all candidate constructions of such groups have very large sizes, leading to inefficient proofs compared to the state-of-the-art. These recent works overcame this issue and work over standard cryptographic groups, but only achieved a \emph{relaxed} form of security, which unfortunately limits their application (for example, this relaxed security does not suffice in anonymous transaction applications). A natural, but non-trivial goal is to remove this relaxation. This will require designing new techniques to manipulate encodings of bounded integers over cryptographic groups while preserving their homomorphic properties (\emph{i.e.} the ability to add and multiply encodings, and get an encoding of the result). Another non-trivial goal would be to extend these techniques to handle membership proofs for sets more complex than intervals. A possible way to achieve this would be to identify a natural family of useful sets which have a compact description by Diophantine equations, possibly by building upon the techniques used in the classical theorem of Matiyasevich (previously used in cryptography in~\cite{AC:Lipmaa03a}) which characterizes all recursively enumerable sets by Diophantine equations.

\qboxEff{Can we construct efficient \emph{post-quantum} non-interactive zero-knowledge proofs for the above statements?}

\paragraph{Methodology.} The past work of the PI on silent preprocessing in secure computation led to the development of new techniques to use coding-theoretic assumptions in the design of efficient secure computation protocols. The MPC-in-the-head technique~\cite{STOC:IKOS07} is a general compilation technique that can be used to transform secure computation protocols (in the honest-but-curious setting) into zero-knowledge proofs. Recently, this compiler has led to many new efficient constructions~\cite{USENIX:GiaMadOrl16,CCS:KatKolWan18}. Within the project \OBELiSC, the PI will introduce new non-interactive zero-knowledge proofs based on coding-theoretic assumptions (which are among the most promising post-quantum assumptions) by designing new coding-theoretic secure computation protocols for statements of interest (such as range proofs, membership proofs, or proofs of knowledge of a preimage to a hash function). When designing these protocols, the parameter to optimize will be the \emph{size of the ZKP obtained} when applying the MPC-in-the-head compiler on the protocol. This changes significantly the target efficiency measures, and will likely require the introduction of new coding-theoretic techniques tailored to this parameter.

\paragraph{Theoretical security foundations.} The second target of \OBELiSC will be the theoretical security foundations of non-interactive zero-knowledge. An important goal in cryptography is to relate the security of the most important objects to the most plausible and best-studied assumptions. The more a primitive can be built from a variety of standard assumptions, the more confidence we gain in its ability to withstand unforeseen algorithmic or technological breakthroughs. This program, introduced by Goldwasser and Micali in 1982~\cite{STOC:GolMic82}, has had great success so far, and hundreds of researchers have built a network of provable security reductions relating all standard primitives to standard assumptions. Among them, NIZKs proved to be one of the toughest goals: after initial results under factorization-style assumptions~\cite{STOC:BluFelMic88} and pairing-based assumptions~\cite{EC:GroSah08}, NIZKs resisted for a long time all attempts to base their security on established public-key assumptions. As we outlined in Section~\ref{subsec:ni}, several barriers finally fell recently. However, some important links are still missing, such as the relation between NIZKs and the decisional Diffie-Hellman assumption (currently, a reduction is known, but requires sub-exponential hardness~\cite{EC:JaiJin21}). Most importantly, the core fundamental question of whether \emph{public-key cryptography is at all necessary} for NIZKs, remains wide open.

\qboxSec{Can we base the security of non-interactive zero-knowledge proofs on traditional hardness assumptions such as the decisional or computation Diffie-Hellman assumptions?}

\paragraph{Methodology.} The latest advances in NIZKs used an object called \emph{correlation-intractable hash function} (CIHF), that provides ``full-fledged'' NIZKs, but currently under strong assumptions (e.g. assuming hardness against \emph{subexponential}, rather than polynomial, adversaries~\cite{EC:JaiJin21}). On the other hand, the notion of verifiable pseudorandom generators (VPRG)~\cite{FOCS:DwoNao00} has been used by the PI and others~\cite{EC:KNYY19,EC:CouHof19,EC:QuaRotWic19} to introduce ``almost'' NIZKs, under weak assumptions, such as the computational Diffie-Hellman assumption. A natural direction is to investigate whether a careful combination of the two primitives can provide a best-of-both-world result: the weak assumptions of the VPRG-based approach, with the full-fledged security of the CIHF-based approach. One possible way to reconcile these approaches is to use VPRGs to reduce the existence of NIZKs of NP to the existence of NIZKs for a simple, minimal language, and afterward to use CIHF to design a NIZK for this language, with the hope that this simplicity will allow for a design of CIHF under more standard assumptions. Another, more intriguing approach is to investigate \emph{disjunction} strategies: showing that the \emph{impossibility} of building CIHF under standard assumptions would imply the \emph{possibility} of building VPRGs powerful enough to provide full-fledged NIZKs, hence demonstrating (non-constructively) that one of these complementary approaches is bound to succeed.

\qboxSec{Is public-key cryptography necessary for non-interactive zero-knowledge proofs? Can we construct NIZKs from assumptions not known to imply public-key cryptography?}

\paragraph{Methodology.} This last question is the most open-ended question of RG2. Whether public-key cryptography is necessary for non-interactive zero-knowledge remained open since the introduction of NIZKs in 1988~\cite{STOC:BluFelMic88}. Because the Fiat-Shamir transform only requires a hash function to convert interactive ZKP into NIZK, and because ZKP do not require public key cryptography, it seems clear that public-key cryptography is \emph{not} needed; yet, \emph{all known} provable constructions of NIZKs for NP, even those that build upon the Fiat-Shamir paradigm, have used public key cryptography. The bottom-up approach will be to start from concrete symmetric assumptions, \emph{e.g.} the existence of collision-resistant hash functions, and to ask: how large is the class of languages that can be shown to admit NIZKs under this assumption? In particular, can we exhibit any such NIZK for a language outside the class of languages known to have \emph{unconditional} NIZKs? The complementary approach is the top-down way: starting from constructions of NIZKs under highly exotic assumptions, either related to the correlation-intractability of hash functions, or discrete-logarithm-style assumption (as the discrete logarithm assumption is not known to imply public key cryptography), and trying to make the underlying assumptions less exotic. A starting point for this approach was introduced by the PI in~\cite{EC:CouKatUrs20}, leading to a NIZK under a highly exotic and very strong variant of the discrete logarithm assumption. While this assumption is technically not known to imply public-key encryption, it is too \emph{ad hoc} and exotic to provide a compelling solution. However, refining the approach and progressively bringing the assumption closer to the standard discrete logarithm assumption might be possible.

\subsubsection{Organization over time}

RG2 will span over the last four years of project \OBELiSC (year~2 to year~5). Three PhD students will be involved: one from year~2 to year~4 on silent preprocessing aspects in RG1 and RG2 (Sections~\ref{subsub:ole},~\ref{subsub:pcf}, and~\ref{subsub:nisp}), one from year~3 to year~5 on practical and theoretical aspects of zero-knowledge proofs (Section~\ref{subsub:nizk}), and one from year~3 to year~5 on cryptography with low-communication overhead, including some sub-goals of Section~\ref{subsub:nizk} related to achieving low-communication zero-knowledge proofs, as well as the goals in Section~\ref{subsub:comm} of RG3. Depending on the candidates, a postdoc might be involved in these aspects.

\subsubsection{Risks and impacts}

NISC for silent preprocessing fits perfectly within the expertise of the PI. In addition, this goal might span new collaborations of the PI with some of its regular coauthors on the silent preprocessing line of work, such as Yuval Ishai, Elette Boyle, Niv Gilboa, Lisa Kohl, Peter Scholl, and Peter Rindal. Yet, it remains especially challenging, and its success will much likely be tied to the successful introduction of new silent preprocessing protocols in RG1, as it will yield new strategies to circumvent the strong limitations of the only known approach from~\cite{EC:OrlSchYak21}. Its success would be the \textbf{third core milestone} (after the development of efficient two-party silent preprocessing for all functionalities, and that of efficient $n$-party silent preprocessing) on the path to the ultimate goal of \OBELiSC, a world where privacy-by-default is a reality on large networks.

We expect the design of new efficient NIZKs to have a strong potential for \textbf{immediate impact}. For example, efficient range proofs are a core component in \textbf{widely-used cryptocurrencies} such as Monero~\cite{monero}. Post-quantum NIZKs are likely to become increasingly impactful as our cryptographic ecosystem moves progressively to quantum-proof solutions. In particular, new efficient code-based NIZKs can yield new post-quantum signatures, which could be \textbf{submitted for consideration by the NIST} in the context of the new post-quantum standards. The PI plans to collaborate on this goal with renowned experts on coding theory, cryptanalysis, and the design of signatures, such as Antoine Joux (co-supervisor of Eliana Carozza, a PhD student of the PI). The PI is an expert on range proofs, and the expertise on code-based cryptography developed in the context of silent secure computation will be instrumental in introducing new approaches to efficient code-based NIZKs. The impacts of the result of \OBELiSC on the security foundations of NIZKs will be of a more theoretical nature but remain nonetheless of fundamental interest. The necessity of public-key cryptography is the most challenging of all questions in RG2 and carries the most risk, but we expect the top-down and bottom-up approaches we outlined to at least yield partial results in this direction.

\subsection{Research Goal 3 (RG3): Low communication secure computation}

As we explained in Section~\ref{subsub:lowcomm}, the chosen angle in \OBELiSC is to investigate the existence of new approaches to low-communication secure computation, that do not rely on fully homomorphic encryption (FHE). The rationale behind this choice is (1) FHE is heavily studied in countless projects, by many researchers and companies, while alternative approaches are comparatively neglected; (2) FHE is very expensive, and using other primitives might be a path towards achieving better efficiency; (3) FHE is only known under a specific assumption (the LWE assumption), while other approaches help to diversify the assumptions implying low-communication secure computation, strengthening its security foundations; (4) FHE-based secure computation does not fit in the silent preprocessing paradigm, while a central thesis of \OBELiSC is that this feature is one of the fundamental features to achieve towards deploying secure computation over large networks.

Previous works of the PI have shown that sublinear-communication secure computation can be achieved from new assumptions, and within the silent preprocessing paradigm~\cite{EC:Couteau19,EC:CouMey21}. Homomorphic secret sharing has been suggested as an alternative to FHE in~\cite{C:BoyGilIsh16} to achieve low-communication secure computation, and a recent work of the PI~\cite{TCC:BoyCouMey22} identified \emph{decomposable batch oblivious transfer} as a third, incomparable route to sublinear-communication secure computation. Yet, these works remained so far mostly of theoretical interest, and lack concrete efficiency. Furthermore, they have been restricted so far to specific settings: they work solely in the two-party case, and only for structured circuits (essentially, the circuit must have a layered structure, where all wires of the circuit only connect adjacent layers). Eventually, they only go mildly beyond the linear communication barrier: all known FHE-free constructions communicate at least $O(s/\log s)$ bits for circuits of size $s$. In the silent preprocessing paradigm, the situation is even worse: the only known protocol~\cite{EC:CouMey21} is extremely inefficient, and communicates $O(s/\log\log s)$ bits, shaving only a $\log\log s$ factor compared to the standard (and efficient) linear-communication solutions. The goal of RG3 will be to overcome all of these shortcomings and establish non-FHE-based techniques as promising approaches to get concretely usable low-communication secure computation.

\iffalse
It is known that the silent generation of specific types of preprocessing material suffices to enable sublinear secure computation. A first natural goal is to refine the construction of sublinear communication protocols in the silent preprocessing paradigm, either by developing better protocols for generating this preprocessing material or by identifying other types of preprocessing material that yield better sublinear communication protocols.

\qboxEff{Can we construct a more efficient silent preprocessing protocol for generating one-time truth-table correlations, or subset tensor product correlations? Is there a better correlation to achieve sublinear secure computation?}

\paragraph{Methodology.} The one-time truth-table correlation was shown in~\cite{EC:Couteau19} to suffice for sublinear-communication secure computation with preprocessing. However, no silent preprocessing protocol is known for this correlation. Instead, the work of~\cite{EC:CouMey21} introduces a related correlation, called the \emph{subset tensor product correlation}, and shows that it could be generated silently under coding-theoretic assumptions and that it suffices to achieve sublinear secure computation. the goal in~\cite{EC:CouMey21} was to demonstrate that coding-theoretic assumptions imply low-communication secure computation. It is likely that alternative techniques, building upon the homomorphic secret sharing schemes of~\cite{C:BoyGilIsh16, EC:BoyKohSch19} or~\cite{EC:OrlSchYak21} could be used to generate subset tensor product correlations (or even one-time truth-table correlations) with much greater efficiency. A starting point would be to investigate these HSS-based constructions in the context of subset tensor products, and optimize their efficiency.
\fi

\iffalse
\qboxSec{Can we achieve sublinear secure computation under the quadratic residuosity assumption?}
\fi

\subsubsection{Handling more parties and more functions}
\label{subsub:more}

Ultimately, large-scale secure computation should enjoy low communication for all functions (and not only structured ones), and possibly large groups of participants (as opposed to two-party functionalities). Yet, both are currently only known under FHE, whose efficiency limitations remain prohibitive. Therefore, \OBELiSC will investigate alternative methods to circumvent these limitations.

\qboxFea{Can we achieve sublinear secure computation between $3$ or more parties without fully homomorphic encryption?}

\paragraph{Methodology.} FHE-based secure computation enjoys two-types of orthogonal low communication features: it enables \emph{sublinearity in the function size} for general functions, but also \emph{sublinearity in the input size} for simple functions (\emph{e.g.} simple queries on large databases). Interestingly, each property can be achieved \emph{individually} without FHE: the former with HSS~\cite{C:BoyGilIsh16}, and the latter with private information retrieval (PIR)~\cite{FOCS:CGKS95}. A natural direction is to study how they can be combined. Imagine for example three parties $A,B,C$ with inputs $x,y,z$. An approach to evaluate $f(x,y,z)$ could be: $A$ and $B$ somehow evaluate the \emph{partial function} $f(x,y,\cdot)$ using a two-party low communication protocol (\emph{e.g.} using HSS). Now, this description of $f(x,y,\cdot)$ (a truth table) could be used as a very large \emph{input} to a PIR-based protocol with $C$, where $C$ queries $f(x,y,\cdot)$ at her entry $z$.

\qboxFea{Can we achieve sublinear secure computation for all polynomial-size circuit (rather than layered circuits) without fully homomorphic encryption?}

\paragraph{Methodology.} Abstracting out, the restriction to layered circuits of all known constructions stems from the fact that they require a core property: one can \emph{mark} a sublinear-size subset of nodes on the graph such that every gate of the graph is the descendent of a ``relatively small'' subset of marked nodes. This property is easy to achieve for layered circuits. For general circuits, however, it becomes a much harder problem, connected to the notion of \emph{depth-robust} graphs~\cite{EC:AlwBloPie17}, studied in cryptography in the context of memory-hard functions (where removing many nodes cannot reduce too much the depth of the resulting circuit), and to the concept of \emph{grates}~\cite{schnitger1983depth}, an old type of graphs which has been used to prove complexity-theoretic lower bounds on Turing machines. This question is therefore heavily graph-theoretic, but for a very special family of graphs: the family of circuits (\emph{i.e.} directed acyclic graphs of indegree two). The PI will team up with the graph theory group of IRIF to investigate whether the restriction to circuit-graphs allows improving on the existing methods to construct grates, towards allowing to identify a sublinear-size set of marked nodes with the right properties. If successful, this would imply FHE-free sublinear secure computation for all polynomial-size circuits.

\subsubsection{Reducing the communication}
\label{subsub:comm}
Eventually, we outline two of the most intriguing questions in low-communication secure computation: can FHE-free approaches achieve better savings? This seems to be a necessary first step towards achieving concretely efficient protocol. Indeed, the current sublinearity factor of known constructions is either $\log s$ or even $\log \log s$, which does not translate to strong communication savings in practice. Furthermore, this limitation stem from the fact that known techniques ``pay'' a cost that is either exponential or even \emph{doubly} exponential, in the sublinearity factor, making them computationally heavy. Breaking these sublinearity barriers would therefore necessarily improve both communication and computation.

\qboxEff{Can we break the $O(s/\log s)$ communication barrier for secure computation without fully homomorphic encryption? Can we  break the $O(s/\log s\log s)$ communication barrier for secure computation with silent preprocessing?}

\paragraph{Methodology.} Both questions are wide open, with no clear path toward positive results. The most natural solution would be to construct homomorphic secret sharing for all polynomial-size circuits, but no known construction comes close to that. This is a long-term, high-risk open question. The project \OBELiSC will investigate many paths to develop new tools for low-communication secure computation, outlined in the previous methodology sections. These new tools might help get a better understanding of why all known FHE-free methods are stuck at these slightly-sublinear barriers, which will be the first step in finding a way around them.

\subsubsection{Organization over time}

RG3 will span all five years of project \OBELiSC. Two PhD students will work on RG3, one with a focus on secure computation at scale (Section~\ref{subsub:more}, as well as Section~\ref{subsub:msp} of RG1) from year~1 to year~3, and one from year~3 to year~5 on cryptography with low-communication overhead (Section~\ref{subsub:comm}, plus some sub-goals of Section~\ref{subsub:nizk}). Depending on the candidates, a postdoc might be involved in these aspects.

\subsubsection{Risks and impacts}

The biggest risk of RG3 is to fail to yield promising \emph{practical} low-communication protocols. The goals of RG3 are shaped by the PI's intuition that the path to efficient low-communication secure computation must go through the exploration of alternatives to FHE. Previous results in this direction, both by the PI and by others, have been promising \emph{theoretical} feasibility results, but they have yet to demonstrate their potential for practicality -- and strong barriers must be lifted before this can happen. In a way, this is not dissimilar to the situation for FHE a decade ago, and the latter is now at a point where it makes sense for companies to develop FHE-based solutions. Exploring RG3 is therefore a \textbf{carefully calculated gamble}: it will most likely yield new theoretical results, broadening our understanding of the feasibility of low-communication secure computation, and it has the potential to reveal new avenues towards practical low-communication secure computation, but it is not guaranteed to achieve the latter.

The PI has published most of the recent results in this area~\cite{EC:Couteau19,EC:CouMey21,TCC:BoyCouMey22}, and is therefore well equipped to pursue this challenging goal. If RG3 succeeds in yielding protocols with the potential to become practical, this would reconcile low-communication secure computation with the other crucial features targeted in \OBELiSC, such as silent preprocessing. It would demonstrate that privacy-by-default on large-scale networks can be achieved without incurring a prohibitive bandwidth overhead, the \textbf{last major milestone} on the path to the ultimate goal of \OBELiSC.

\iffalse
\section*{Section c. Description of the Resources}
\setcounter{subsection}{0}

The basis for the project will be the expertise of the PI. During the first year of the project, it will also benefit from the expertise of three PhD students supervised by the PI, which will at the time enter the last year of their PhD, and from the expertise of at least one postdoc, who is starting a two-year contract. This team will provide the foundation for the project. Because of its magnitude, the project will also require involving new PhD students and postdocs. The team will assist the PI in educating the new PhD students to research in general, and to some of the core scientific fields involved in this proposal in particular. We will also pursue and develop our fruitful collaborations with international partners.

\subsection{Core Team Members}

The core team member is the PI, Geoffroy Couteau, an expert on secure computation, zero-knowledge proofs, code-based cryptography, and theoretical foundations of cryptography. It also includes three of the PI's current PhD students: Dung Bui, Clment Ducros, and Eliana Carozza, and at least one postdoc, Christoph Egger. Two other postdocs, Alexander Koch and Sven Maier, might be involved in the first year of the project if their contract is extended. The team will also include the new PhD students and postdocs directly funded by the project.

\subsection{Associate Team Members}

Other French researchers will be closely associated with the project: Adi Rosn, (CNRS research director at FILOFOCS and IRIF, an expert on communication complexity, randomness complexity, and information-theoretic secure computation), Alain Couvreur (INRIA researcher at the LIX, co-supervisor of Clment Ducros, and an expert on algebraic coding theory and code-based cryptography), and Antoine Joux (tenured-faculty at CISPA (Helmholtz center for Cybersecurity), co-supervisor of Eliana Carozza, and a world-renowned expert on cryptanalysis).

\subsection{Collaborators}

Our numerous international collaborations will continue, in particular with the Technion (Yuval Ishai), NTT research (Elette Boyle), Ben-Gurion University (Niv Gilboa), Aarhus University (Peter Scholl), CWI and University of Amsterdam (Lisa Kohl, Nicolas Resch), Visa Research (Peter Rindal, Srinivasan Raghuraman), AIST Japan (Shuichi Katsumata), and Aalto University (Chris Brzuska).

\subsection{Requested Resources}

The requested resources are essentially for personnel, including salaries and visits (travel and per-diem for international visitors, visits abroad for international collaborations). The members of the project will also attend international conferences to present the results achieved in the context of this project, and we plan to organize a workshop. The resources should also cover basic equipment for the personnel (\emph{e.g.} computers for the PhD students).

The personnel costs, and their distribution over the years and within the research goals of \OBELiSC, are summarized in Figure~\ref{fig:distribution}. The project will involve 4 PhD students, and one post-doc during the last four years of the project. The post descriptions are given below:\\

\begin{itemize}
    \item PhD Student 1:
    \begin{itemize}
        \item Title: Scalability of secure computation
        \item Task: This doctoral work will be related to RG1 and RG3. The goal is to study $n$-party secure computation in the setting where $n$ can be large. The work will investigate the setting of $n$-party silent preprocessing, the communication complexity of $n$-party computation, through the lens of its dependency in $n$.
    \end{itemize}
    \item PhD Student 2: 
        \begin{itemize}
        \item Title: Silent secure computation
        \item Task: This doctoral work will be related to RG1 and RG2. The goal is to study methods to improve silent preprocessing protocol in terms of expressivity, efficiency, security, and interactivity.
    \end{itemize}
    \item PhD Student 3: 
        \begin{itemize}
        \item Title: Security and efficiency of zero-knowledge cryptography
        \item Task: This doctoral work will be related to RG2. The goal is to study the design of new zero-knowledge proofs, for general statements or concrete statements, with or without post-quantum security. The thesis will consider both aspects of concrete efficiency and theoretical foundations.
    \end{itemize}
    \item PhD Student 4: 
        \begin{itemize}
        \item Title: Cryptography with low communication overhead
        \item Task: This doctoral work will be related to RG2 and RG3. The goal is to develop a new method to achieve cryptographic primitives with sublinear communication, in the setting of zero-knowledge proofs and secure computation.
    \end{itemize}
    \item Postdoc: during the last four years of the project, post-docs with a strong expertise in secure computation and zero-knowledge proofs will collaborate with the PI and the PhD students. The post description will depend on the state of the project and the duration of the contracts.
\end{itemize}
\fi
\end{alphasection}
%%% Bibliography
\begin{small}
  \bibliographystyle{splncs03}
\bibliography{cryptobib/abbrev3,cryptobib/crypto,add}
\end{small}

\end{document}
